{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df80aee1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb4c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f509007f790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    parallel = True\n",
    "else:\n",
    "    parallel = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e152f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "lr = 1e-5\n",
    "\n",
    "# Other Parameters\n",
    "class_label_map = {\"severe\": 2, \"mild\": 1, \"little_or_none\": 0}\n",
    "# class_label_map = {\"landslide\":6, \"other_disaster\":5, \"fire\":4, \"hurricane\":3, \"earthquake\":2, \"flood\":1, \"not_disaster\":0}\n",
    "# class_label_map = {\"affected_injured_or_dead_people\": 3, \"rescue_volunteering_or_donation_effort\": 2, \"infrastructure_and_utility_damage\": 1, \"not_humanitarian\": 0}\n",
    "# class_label_map = {\"informative\": 1, \"not_informative\": 0}\n",
    "\n",
    "num_classes = len(class_label_map)\n",
    "task = 'Damage_Severity_Classification' \n",
    "# task = 'Disaster_Types_Classification'\n",
    "# task = 'Humanitarian_Classification'\n",
    "# task = 'Informativeness_Classification'\n",
    "\n",
    "weights_denet121 = models.DenseNet121_Weights.IMAGENET1K_V1\n",
    "weights_effnetb1 = models.EfficientNet_B1_Weights.IMAGENET1K_V1\n",
    "weights_mobnetv2 = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "weights_resnet18 = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "weights_resnet50 = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "weights_resnet101 = models.ResNet101_Weights.IMAGENET1K_V1\n",
    "weights_vgg16 = models.VGG16_Weights.IMAGENET1K_V1\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project='Reproduce_exp', \n",
    "           config={\n",
    "               \"learning_rate\": lr,\n",
    "               \"epochs\": epochs,\n",
    "               \"batch_size\": batch_size,\n",
    "           },\n",
    "           name='reproduce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767ee1e",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1182fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join('/work/u9562361/crisis_vision_benchmarks/Damage Severity', \"train\"), \n",
    "                                    transform = transforms.Compose([\n",
    "                                        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        tr_normalize,\n",
    "                                    ]))\n",
    "\n",
    "test_dataset = datasets.ImageFolder(os.path.join('/work/u9562361/crisis_vision_benchmarks/Damage Severity', \"val\"))\n",
    "dev_dataset = datasets.ImageFolder(os.path.join('/work/u9562361/crisis_vision_benchmarks/Damage Severity', \"dev\"))\n",
    "\n",
    "test_dataset.transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    tr_normalize,\n",
    "])\n",
    "\n",
    "dev_dataset.transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    tr_normalize,\n",
    "])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "devloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89390566",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcabad7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = models.densenet121(weights=weights_denet121)\n",
    "# model = models.efficientnet_b1(weights=weights_effnetb1)\n",
    "model = models.mobilenet_v2(weights=weights_mobnetv2)\n",
    "# model = models.resnet18(weights=weights_resnet18)\n",
    "# model = models.resnet50(weights=weights_resnet50)\n",
    "# model = models.resnet101(weights=weights_resnet101)\n",
    "# model = models.vgg16(weights=weights_vgg16)\n",
    "\n",
    "# DenseNet Architecture\n",
    "# num_ftrs = model.classifier.in_features\n",
    "# model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# EfficientNet, MobileNet, VGG Architecture\n",
    "num_ftrs = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# ResNet Architecture\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "model = DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, mode='max')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_logs = {\"loss\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1-score\": [], \"time\": []}\n",
    "val_logs = {\"loss\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1-score\": [], \"time\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f141122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning schedule update\n",
    "def dev_one_epoch(dev_data_loader):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dev_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(images)  # Forward\n",
    "\n",
    "            # Calculating Loss\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            # Calculating Accuracy\n",
    "            _, predicts = preds.max(1)\n",
    "            predicts = predicts.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            acc = accuracy_score(labels, predicts)\n",
    "            epoch_acc.append(acc)\n",
    "\n",
    "    # Acc and Loss\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3974d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_data_loader):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    epoch_ps = []\n",
    "    epoch_rs = []\n",
    "    epoch_f1 = []\n",
    "    trues = []\n",
    "    prediction = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate (train_data_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        # Calculating Loss\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        # Calculating Metrics\n",
    "        _, predicts = preds.max(1)\n",
    "        predicts = predicts.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        acc = accuracy_score(labels, predicts)\n",
    "        ps = precision_score(labels, predicts, average=\"weighted\")\n",
    "        rs = recall_score(labels, predicts, average=\"weighted\")\n",
    "        f1 = f1_score(labels, predicts, average=\"weighted\")\n",
    "        \n",
    "        epoch_acc.append(acc)\n",
    "        epoch_ps.append(ps)\n",
    "        epoch_rs.append(rs)\n",
    "        epoch_f1.append(f1)\n",
    "        trues.append(labels)\n",
    "        prediction.append(predicts)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    trues = np.concatenate(trues)\n",
    "    prediction = np.concatenate(prediction)\n",
    "    accuracy = accuracy_score(trues, prediction)\n",
    "    precision = precision_score(trues, prediction, average=\"weighted\")\n",
    "    recall = recall_score(trues, prediction, average=\"weighted\")\n",
    "    f1score = f1_score(trues, prediction, average=\"weighted\")\n",
    "    accuracy = accuracy * 100\n",
    "    precision = precision * 100\n",
    "    recall = recall * 100\n",
    "    f1score = f1score * 100\n",
    "\n",
    "    # Overall Epoch Results\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc) * 100\n",
    "    epoch_ps = np.mean(epoch_ps) * 100\n",
    "    epoch_rs = np.mean(epoch_rs) * 100\n",
    "    epoch_f1 = np.mean(epoch_f1) * 100\n",
    "\n",
    "    train_logs[\"loss\"].append(epoch_loss)\n",
    "    train_logs[\"accuracy\"].append(accuracy)\n",
    "    train_logs[\"precision\"].append(precision)\n",
    "    train_logs[\"recall\"].append(recall)\n",
    "    train_logs[\"f1-score\"].append(f1score)\n",
    "    train_logs[\"time\"].append(total_time)\n",
    "    wandb.log({\"train_loss\": epoch_loss, \"train_f1\": f1score})\n",
    "\n",
    "    return epoch_loss, accuracy, precision, recall, f1score, total_time\n",
    "\n",
    "def val_one_epoch(val_data_loader, best_val_f1):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    epoch_ps = []\n",
    "    epoch_rs = []\n",
    "    epoch_f1 = []\n",
    "    trues = []\n",
    "    prediction = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "\n",
    "            # Calculating Loss\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            # Calculating Metrics\n",
    "            _, predicts = preds.max(1)\n",
    "            predicts = predicts.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            acc = accuracy_score(labels, predicts)\n",
    "            ps = precision_score(labels, predicts, average=\"weighted\")\n",
    "            rs = recall_score(labels, predicts, average=\"weighted\")\n",
    "            f1 = f1_score(labels, predicts, average=\"weighted\")\n",
    "            epoch_acc.append(acc)\n",
    "            epoch_ps.append(ps)\n",
    "            epoch_rs.append(rs)\n",
    "            epoch_f1.append(f1)\n",
    "            trues.append(labels)\n",
    "            prediction.append(predicts)\n",
    "    \n",
    "    trues = np.concatenate(trues)\n",
    "    prediction = np.concatenate(prediction)\n",
    "    accuracy = accuracy_score(trues, prediction)\n",
    "    precision = precision_score(trues, prediction, average=\"weighted\")\n",
    "    recall = recall_score(trues, prediction, average=\"weighted\")\n",
    "    f1score = f1_score(trues, prediction, average=\"weighted\")\n",
    "    accuracy = accuracy * 100\n",
    "    precision = precision * 100\n",
    "    recall = recall * 100\n",
    "    f1score = f1score * 100\n",
    "    cm = confusion_matrix(trues, prediction)\n",
    "    cr = classification_report(trues, prediction)\n",
    "\n",
    "    # Overall Epoch Results\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc) * 100\n",
    "    epoch_ps = np.mean(epoch_ps) * 100\n",
    "    epoch_rs = np.mean(epoch_rs) * 100\n",
    "    epoch_f1 = np.mean(epoch_f1) * 100\n",
    "\n",
    "    val_logs[\"loss\"].append(epoch_loss)\n",
    "    val_logs[\"accuracy\"].append(accuracy)\n",
    "    val_logs[\"precision\"].append(precision)\n",
    "    val_logs[\"recall\"].append(recall)\n",
    "    val_logs[\"f1-score\"].append(f1score)\n",
    "    val_logs[\"time\"].append(total_time)\n",
    "    wandb.log({\"val_loss\": epoch_loss, \"val_f1\": f1score})\n",
    "\n",
    "    # Saving best model\n",
    "    if f1score > best_val_f1:\n",
    "        best_val_f1 = f1score\n",
    "        torch.save(model.state_dict(), \"reproduce.pth\")\n",
    "\n",
    "    return epoch_loss, accuracy, precision, recall, f1score, total_time, best_val_f1, cm, cr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4c892",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64650bf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_val_ps = 0\n",
    "best_val_rs = 0\n",
    "best_val_f1 = 0\n",
    "\n",
    "output_list = [] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, train_ps, train_rs, train_f1, train_time = train_one_epoch(trainloader) \n",
    "    dev_loss, dev_acc = dev_one_epoch(devloader) \n",
    "    val_loss, val_acc, val_ps, val_rs, val_f1, val_time, best_val_f1, cm, cr = val_one_epoch(testloader, best_val_f1) \n",
    "    \n",
    "    lr_scheduler.step(dev_acc)\n",
    "\n",
    "    if val_ps > best_val_ps:\n",
    "        best_val_ps = val_ps\n",
    "\n",
    "    if val_rs > best_val_rs:\n",
    "        best_val_rs = val_rs\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "    total_time = train_time + val_time\n",
    "    output_str = f\"Epoch {epoch+1}/{epochs} - loss: {train_loss:.4f} - f1-score: {train_f1:.2f}% - val_loss: {val_loss:.4f} - val_f1-score: {val_f1:.2f}% - time: {total_time:.2f}s\"\n",
    "    output_list.append(output_str)\n",
    "    print(output_str)\n",
    "\n",
    "print()\n",
    "print(task + ' Performance:')\n",
    "print(f'Accuracy : {best_val_acc:.2f}%')\n",
    "print(f'Precision : {best_val_ps:.2f}%')\n",
    "print(f'Recall : {best_val_rs:.2f}%')\n",
    "print(f'F1-Score : {best_val_f1:.2f}%')\n",
    "print()\n",
    "print(task + ' Classification Report:')\n",
    "print(cr)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=None)\n",
    "disp.plot()\n",
    "plt.savefig(\"reproduce-CM.png\") \n",
    "plt.close() \n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
