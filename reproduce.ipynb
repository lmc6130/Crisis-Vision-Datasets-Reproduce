{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df80aee1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb4c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f509007f790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    parallel = True\n",
    "else:\n",
    "    parallel = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e152f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlmc6130\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/u9562361/Reproduce Results/wandb/run-20231025_171729-poskci71</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lmc6130/Reproduce_exp/runs/poskci71' target=\"_blank\">mobilenet-v2-dsreproduce</a></strong> to <a href='https://wandb.ai/lmc6130/Reproduce_exp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lmc6130/Reproduce_exp' target=\"_blank\">https://wandb.ai/lmc6130/Reproduce_exp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lmc6130/Reproduce_exp/runs/poskci71' target=\"_blank\">https://wandb.ai/lmc6130/Reproduce_exp/runs/poskci71</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lmc6130/Reproduce_exp/runs/poskci71?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4fb133e830>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Parameters\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "lr = 1e-5\n",
    "\n",
    "# Other Parameters\n",
    "class_label_map = {\"severe\": 2, \"mild\": 1, \"little_or_none\": 0}\n",
    "# class_label_map = {\"landslide\":6, \"other_disaster\":5, \"fire\":4, \"hurricane\":3, \"earthquake\":2, \"flood\":1, \"not_disaster\":0}\n",
    "# class_label_map = {\"affected_injured_or_dead_people\": 3, \"rescue_volunteering_or_donation_effort\": 2, \"infrastructure_and_utility_damage\": 1, \"not_humanitarian\": 0}\n",
    "# class_label_map = {\"informative\": 1, \"not_informative\": 0}\n",
    "\n",
    "num_classes = len(class_label_map)\n",
    "task = 'Damage_Severity_Classification' \n",
    "# task = 'Disaster_Types_Classification'\n",
    "# task = 'Humanitarian_Classification'\n",
    "# task = 'Informativeness_Classification'\n",
    "\n",
    "weights_denet121 = models.DenseNet121_Weights.IMAGENET1K_V1\n",
    "weights_effnetb1 = models.EfficientNet_B1_Weights.IMAGENET1K_V1\n",
    "weights_mobnetv2 = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "weights_resnet18 = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "weights_resnet50 = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "weights_resnet101 = models.ResNet101_Weights.IMAGENET1K_V1\n",
    "weights_vgg16 = models.VGG16_Weights.IMAGENET1K_V1\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project='Reproduce_exp', \n",
    "           config={\n",
    "               \"learning_rate\": lr,\n",
    "               \"epochs\": epochs,\n",
    "               \"batch_size\": batch_size,\n",
    "           },\n",
    "           name='reproduce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767ee1e",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1182fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join('/work/u9562361/crisis_vision_benchmarks/Damage Severity', \"train\"), \n",
    "                                    transform = transforms.Compose([\n",
    "                                        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        tr_normalize,\n",
    "                                    ]))\n",
    "\n",
    "test_dataset = datasets.ImageFolder(os.path.join('/work/u9562361/crisis_vision_benchmarks/Damage Severity', \"val\"))\n",
    "dev_dataset = datasets.ImageFolder(os.path.join('/work/u9562361/crisis_vision_benchmarks/Damage Severity', \"dev\"))\n",
    "\n",
    "test_dataset.transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    tr_normalize,\n",
    "])\n",
    "\n",
    "dev_dataset.transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    tr_normalize,\n",
    "])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "devloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89390566",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcabad7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = models.densenet121(weights=weights_denet121)\n",
    "# model = models.efficientnet_b1(weights=weights_effnetb1)\n",
    "model = models.mobilenet_v2(weights=weights_mobnetv2)\n",
    "# model = models.resnet18(weights=weights_resnet18)\n",
    "# model = models.resnet50(weights=weights_resnet50)\n",
    "# model = models.resnet101(weights=weights_resnet101)\n",
    "# model = models.vgg16(weights=weights_vgg16)\n",
    "\n",
    "# DenseNet Architecture\n",
    "# num_ftrs = model.classifier.in_features\n",
    "# model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# EfficientNet, MobileNet, VGG Architecture\n",
    "num_ftrs = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# ResNet Architecture\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "model = DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, mode='max')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_logs = {\"loss\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1-score\": [], \"time\": []}\n",
    "val_logs = {\"loss\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1-score\": [], \"time\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f141122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning schedule update\n",
    "def dev_one_epoch(dev_data_loader):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dev_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(images)  # Forward\n",
    "\n",
    "            # Calculating Loss\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            # Calculating Accuracy\n",
    "            _, predicts = preds.max(1)\n",
    "            predicts = predicts.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            acc = accuracy_score(labels, predicts)\n",
    "            epoch_acc.append(acc)\n",
    "\n",
    "    # Acc and Loss\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3974d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_data_loader):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    epoch_ps = []\n",
    "    epoch_rs = []\n",
    "    epoch_f1 = []\n",
    "    trues = []\n",
    "    prediction = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate (train_data_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        # Calculating Loss\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        # Calculating Metrics\n",
    "        _, predicts = preds.max(1)\n",
    "        predicts = predicts.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        acc = accuracy_score(labels, predicts)\n",
    "        ps = precision_score(labels, predicts, average=\"weighted\")\n",
    "        rs = recall_score(labels, predicts, average=\"weighted\")\n",
    "        f1 = f1_score(labels, predicts, average=\"weighted\")\n",
    "        \n",
    "        epoch_acc.append(acc)\n",
    "        epoch_ps.append(ps)\n",
    "        epoch_rs.append(rs)\n",
    "        epoch_f1.append(f1)\n",
    "        trues.append(labels)\n",
    "        prediction.append(predicts)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    trues = np.concatenate(trues)\n",
    "    prediction = np.concatenate(prediction)\n",
    "    accuracy = accuracy_score(trues, prediction)\n",
    "    precision = precision_score(trues, prediction, average=\"weighted\")\n",
    "    recall = recall_score(trues, prediction, average=\"weighted\")\n",
    "    f1score = f1_score(trues, prediction, average=\"weighted\")\n",
    "    accuracy = accuracy * 100\n",
    "    precision = precision * 100\n",
    "    recall = recall * 100\n",
    "    f1score = f1score * 100\n",
    "\n",
    "    # Overall Epoch Results\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc) * 100\n",
    "    epoch_ps = np.mean(epoch_ps) * 100\n",
    "    epoch_rs = np.mean(epoch_rs) * 100\n",
    "    epoch_f1 = np.mean(epoch_f1) * 100\n",
    "\n",
    "    train_logs[\"loss\"].append(epoch_loss)\n",
    "    train_logs[\"accuracy\"].append(accuracy)\n",
    "    train_logs[\"precision\"].append(precision)\n",
    "    train_logs[\"recall\"].append(recall)\n",
    "    train_logs[\"f1-score\"].append(f1score)\n",
    "    train_logs[\"time\"].append(total_time)\n",
    "    wandb.log({\"train_loss\": epoch_loss, \"train_f1\": f1score})\n",
    "\n",
    "    return epoch_loss, accuracy, precision, recall, f1score, total_time\n",
    "\n",
    "def val_one_epoch(val_data_loader, best_val_f1):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    epoch_ps = []\n",
    "    epoch_rs = []\n",
    "    epoch_f1 = []\n",
    "    trues = []\n",
    "    prediction = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "\n",
    "            # Calculating Loss\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            # Calculating Metrics\n",
    "            _, predicts = preds.max(1)\n",
    "            predicts = predicts.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            acc = accuracy_score(labels, predicts)\n",
    "            ps = precision_score(labels, predicts, average=\"weighted\")\n",
    "            rs = recall_score(labels, predicts, average=\"weighted\")\n",
    "            f1 = f1_score(labels, predicts, average=\"weighted\")\n",
    "            epoch_acc.append(acc)\n",
    "            epoch_ps.append(ps)\n",
    "            epoch_rs.append(rs)\n",
    "            epoch_f1.append(f1)\n",
    "            trues.append(labels)\n",
    "            prediction.append(predicts)\n",
    "    \n",
    "    trues = np.concatenate(trues)\n",
    "    prediction = np.concatenate(prediction)\n",
    "    accuracy = accuracy_score(trues, prediction)\n",
    "    precision = precision_score(trues, prediction, average=\"weighted\")\n",
    "    recall = recall_score(trues, prediction, average=\"weighted\")\n",
    "    f1score = f1_score(trues, prediction, average=\"weighted\")\n",
    "    accuracy = accuracy * 100\n",
    "    precision = precision * 100\n",
    "    recall = recall * 100\n",
    "    f1score = f1score * 100\n",
    "    cm = confusion_matrix(trues, prediction)\n",
    "    cr = classification_report(trues, prediction)\n",
    "\n",
    "    # Overall Epoch Results\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc) * 100\n",
    "    epoch_ps = np.mean(epoch_ps) * 100\n",
    "    epoch_rs = np.mean(epoch_rs) * 100\n",
    "    epoch_f1 = np.mean(epoch_f1) * 100\n",
    "\n",
    "    val_logs[\"loss\"].append(epoch_loss)\n",
    "    val_logs[\"accuracy\"].append(accuracy)\n",
    "    val_logs[\"precision\"].append(precision)\n",
    "    val_logs[\"recall\"].append(recall)\n",
    "    val_logs[\"f1-score\"].append(f1score)\n",
    "    val_logs[\"time\"].append(total_time)\n",
    "    wandb.log({\"val_loss\": epoch_loss, \"val_f1\": f1score})\n",
    "\n",
    "    # Saving best model\n",
    "    if f1score > best_val_f1:\n",
    "        best_val_f1 = f1score\n",
    "        torch.save(model.state_dict(), \"reproduce.pth\")\n",
    "\n",
    "    return epoch_loss, accuracy, precision, recall, f1score, total_time, best_val_f1, cm, cr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4c892",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64650bf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 - loss: 0.7261 - f1-score: 66.52% - val_loss: 0.6915 - val_f1-score: 66.81% - time: 237.83s\n",
      "Epoch 2/150 - loss: 0.5759 - f1-score: 74.41% - val_loss: 0.6511 - val_f1-score: 70.07% - time: 105.29s\n",
      "Epoch 3/150 - loss: 0.5314 - f1-score: 76.80% - val_loss: 0.6323 - val_f1-score: 72.35% - time: 103.28s\n",
      "Epoch 4/150 - loss: 0.5019 - f1-score: 78.10% - val_loss: 0.6240 - val_f1-score: 72.22% - time: 104.07s\n",
      "Epoch 5/150 - loss: 0.4789 - f1-score: 79.47% - val_loss: 0.6190 - val_f1-score: 72.84% - time: 103.86s\n",
      "Epoch 6/150 - loss: 0.4577 - f1-score: 80.53% - val_loss: 0.6187 - val_f1-score: 73.04% - time: 102.90s\n",
      "Epoch 7/150 - loss: 0.4381 - f1-score: 81.47% - val_loss: 0.6226 - val_f1-score: 73.42% - time: 103.10s\n",
      "Epoch 8/150 - loss: 0.4160 - f1-score: 82.60% - val_loss: 0.6215 - val_f1-score: 73.64% - time: 102.67s\n",
      "Epoch 9/150 - loss: 0.3977 - f1-score: 83.25% - val_loss: 0.6269 - val_f1-score: 73.98% - time: 104.45s\n",
      "Epoch 10/150 - loss: 0.3787 - f1-score: 84.27% - val_loss: 0.6392 - val_f1-score: 73.79% - time: 104.21s\n",
      "Epoch 11/150 - loss: 0.3583 - f1-score: 85.02% - val_loss: 0.6495 - val_f1-score: 74.21% - time: 103.07s\n",
      "Epoch 12/150 - loss: 0.3405 - f1-score: 86.00% - val_loss: 0.6662 - val_f1-score: 73.88% - time: 103.03s\n",
      "Epoch 13/150 - loss: 0.3220 - f1-score: 87.01% - val_loss: 0.6803 - val_f1-score: 74.00% - time: 103.91s\n",
      "Epoch 14/150 - loss: 0.3016 - f1-score: 87.75% - val_loss: 0.7053 - val_f1-score: 73.71% - time: 104.43s\n",
      "Epoch 15/150 - loss: 0.2840 - f1-score: 88.77% - val_loss: 0.7254 - val_f1-score: 73.72% - time: 105.43s\n",
      "Epoch 16/150 - loss: 0.2668 - f1-score: 89.49% - val_loss: 0.7505 - val_f1-score: 73.61% - time: 102.40s\n",
      "Epoch 17/150 - loss: 0.2467 - f1-score: 90.35% - val_loss: 0.7707 - val_f1-score: 73.34% - time: 103.61s\n",
      "Epoch 18/150 - loss: 0.2294 - f1-score: 91.01% - val_loss: 0.7981 - val_f1-score: 73.37% - time: 104.13s\n",
      "Epoch 19/150 - loss: 0.2128 - f1-score: 91.94% - val_loss: 0.8395 - val_f1-score: 73.43% - time: 104.22s\n",
      "Epoch 20/150 - loss: 0.1995 - f1-score: 92.48% - val_loss: 0.8782 - val_f1-score: 73.08% - time: 102.79s\n",
      "Epoch 21/150 - loss: 0.1763 - f1-score: 93.87% - val_loss: 0.8706 - val_f1-score: 73.27% - time: 103.75s\n",
      "Epoch 22/150 - loss: 0.1741 - f1-score: 93.75% - val_loss: 0.8761 - val_f1-score: 73.24% - time: 104.30s\n",
      "Epoch 23/150 - loss: 0.1736 - f1-score: 93.91% - val_loss: 0.8773 - val_f1-score: 72.63% - time: 103.37s\n",
      "Epoch 24/150 - loss: 0.1725 - f1-score: 93.94% - val_loss: 0.8952 - val_f1-score: 72.48% - time: 103.63s\n",
      "Epoch 25/150 - loss: 0.1686 - f1-score: 94.11% - val_loss: 0.8814 - val_f1-score: 72.98% - time: 103.87s\n",
      "Epoch 26/150 - loss: 0.1669 - f1-score: 94.19% - val_loss: 0.8905 - val_f1-score: 72.93% - time: 104.31s\n",
      "Epoch 27/150 - loss: 0.1641 - f1-score: 94.20% - val_loss: 0.9044 - val_f1-score: 72.78% - time: 104.41s\n",
      "Epoch 28/150 - loss: 0.1653 - f1-score: 94.09% - val_loss: 0.9137 - val_f1-score: 72.61% - time: 102.93s\n",
      "Epoch 29/150 - loss: 0.1642 - f1-score: 94.13% - val_loss: 0.9017 - val_f1-score: 73.00% - time: 105.57s\n",
      "Epoch 30/150 - loss: 0.1597 - f1-score: 94.33% - val_loss: 0.9185 - val_f1-score: 72.93% - time: 103.62s\n",
      "Epoch 31/150 - loss: 0.1596 - f1-score: 94.49% - val_loss: 0.9131 - val_f1-score: 72.71% - time: 104.02s\n",
      "Epoch 32/150 - loss: 0.1569 - f1-score: 94.38% - val_loss: 0.9075 - val_f1-score: 73.29% - time: 105.60s\n",
      "Epoch 33/150 - loss: 0.1589 - f1-score: 94.43% - val_loss: 0.9137 - val_f1-score: 72.86% - time: 104.36s\n",
      "Epoch 34/150 - loss: 0.1559 - f1-score: 94.50% - val_loss: 0.9187 - val_f1-score: 73.00% - time: 103.56s\n",
      "Epoch 35/150 - loss: 0.1547 - f1-score: 94.68% - val_loss: 0.9174 - val_f1-score: 73.00% - time: 103.64s\n",
      "Epoch 36/150 - loss: 0.1563 - f1-score: 94.50% - val_loss: 0.9082 - val_f1-score: 73.11% - time: 103.44s\n",
      "Epoch 37/150 - loss: 0.1598 - f1-score: 94.36% - val_loss: 0.9169 - val_f1-score: 72.65% - time: 103.21s\n",
      "Epoch 38/150 - loss: 0.1558 - f1-score: 94.52% - val_loss: 0.9139 - val_f1-score: 72.90% - time: 103.28s\n",
      "Epoch 39/150 - loss: 0.1549 - f1-score: 94.52% - val_loss: 0.9236 - val_f1-score: 73.05% - time: 104.12s\n",
      "Epoch 40/150 - loss: 0.1566 - f1-score: 94.47% - val_loss: 0.9145 - val_f1-score: 73.08% - time: 104.94s\n",
      "Epoch 41/150 - loss: 0.1551 - f1-score: 94.56% - val_loss: 0.9187 - val_f1-score: 73.00% - time: 104.17s\n",
      "Epoch 42/150 - loss: 0.1583 - f1-score: 94.49% - val_loss: 0.9235 - val_f1-score: 72.82% - time: 104.87s\n",
      "Epoch 43/150 - loss: 0.1569 - f1-score: 94.54% - val_loss: 0.9155 - val_f1-score: 72.93% - time: 103.63s\n",
      "Epoch 44/150 - loss: 0.1536 - f1-score: 94.61% - val_loss: 0.9277 - val_f1-score: 72.95% - time: 102.29s\n",
      "Epoch 45/150 - loss: 0.1577 - f1-score: 94.55% - val_loss: 0.9115 - val_f1-score: 72.92% - time: 103.30s\n",
      "Epoch 46/150 - loss: 0.1571 - f1-score: 94.56% - val_loss: 0.9217 - val_f1-score: 72.84% - time: 102.86s\n",
      "Epoch 47/150 - loss: 0.1550 - f1-score: 94.67% - val_loss: 0.9085 - val_f1-score: 73.06% - time: 103.26s\n",
      "Epoch 48/150 - loss: 0.1527 - f1-score: 94.77% - val_loss: 0.9253 - val_f1-score: 72.77% - time: 104.10s\n",
      "Epoch 49/150 - loss: 0.1574 - f1-score: 94.52% - val_loss: 0.9277 - val_f1-score: 72.72% - time: 104.01s\n",
      "Epoch 50/150 - loss: 0.1581 - f1-score: 94.53% - val_loss: 0.9205 - val_f1-score: 73.16% - time: 102.32s\n",
      "Epoch 51/150 - loss: 0.1550 - f1-score: 94.55% - val_loss: 0.9169 - val_f1-score: 72.84% - time: 102.84s\n",
      "Epoch 52/150 - loss: 0.1570 - f1-score: 94.55% - val_loss: 0.9262 - val_f1-score: 72.97% - time: 104.38s\n",
      "Epoch 53/150 - loss: 0.1567 - f1-score: 94.35% - val_loss: 0.9094 - val_f1-score: 73.23% - time: 103.00s\n",
      "Epoch 54/150 - loss: 0.1539 - f1-score: 94.72% - val_loss: 0.9127 - val_f1-score: 72.99% - time: 103.12s\n",
      "Epoch 55/150 - loss: 0.1549 - f1-score: 94.66% - val_loss: 0.9190 - val_f1-score: 72.85% - time: 104.43s\n",
      "Epoch 56/150 - loss: 0.1577 - f1-score: 94.39% - val_loss: 0.9254 - val_f1-score: 73.02% - time: 103.51s\n",
      "Epoch 57/150 - loss: 0.1560 - f1-score: 94.46% - val_loss: 0.9264 - val_f1-score: 72.92% - time: 103.28s\n",
      "Epoch 58/150 - loss: 0.1595 - f1-score: 94.55% - val_loss: 0.9215 - val_f1-score: 72.42% - time: 102.57s\n",
      "Epoch 59/150 - loss: 0.1564 - f1-score: 94.51% - val_loss: 0.9251 - val_f1-score: 72.95% - time: 102.38s\n",
      "Epoch 60/150 - loss: 0.1565 - f1-score: 94.50% - val_loss: 0.9180 - val_f1-score: 72.84% - time: 103.94s\n",
      "Epoch 61/150 - loss: 0.1571 - f1-score: 94.41% - val_loss: 0.9178 - val_f1-score: 72.85% - time: 103.17s\n",
      "Epoch 62/150 - loss: 0.1578 - f1-score: 94.39% - val_loss: 0.9165 - val_f1-score: 73.00% - time: 103.97s\n",
      "Epoch 63/150 - loss: 0.1549 - f1-score: 94.64% - val_loss: 0.9157 - val_f1-score: 73.16% - time: 103.28s\n",
      "Epoch 64/150 - loss: 0.1571 - f1-score: 94.49% - val_loss: 0.9337 - val_f1-score: 73.28% - time: 104.81s\n",
      "Epoch 65/150 - loss: 0.1549 - f1-score: 94.68% - val_loss: 0.9119 - val_f1-score: 72.89% - time: 104.11s\n",
      "Epoch 66/150 - loss: 0.1535 - f1-score: 94.60% - val_loss: 0.9266 - val_f1-score: 72.87% - time: 103.70s\n",
      "Epoch 67/150 - loss: 0.1547 - f1-score: 94.57% - val_loss: 0.9158 - val_f1-score: 72.91% - time: 104.44s\n",
      "Epoch 68/150 - loss: 0.1557 - f1-score: 94.48% - val_loss: 0.9345 - val_f1-score: 72.90% - time: 104.95s\n",
      "Epoch 69/150 - loss: 0.1570 - f1-score: 94.40% - val_loss: 0.9200 - val_f1-score: 72.96% - time: 104.59s\n",
      "Epoch 70/150 - loss: 0.1532 - f1-score: 94.65% - val_loss: 0.9182 - val_f1-score: 72.95% - time: 103.08s\n",
      "Epoch 71/150 - loss: 0.1539 - f1-score: 94.63% - val_loss: 0.9248 - val_f1-score: 72.81% - time: 105.42s\n",
      "Epoch 72/150 - loss: 0.1558 - f1-score: 94.41% - val_loss: 0.9164 - val_f1-score: 72.76% - time: 103.80s\n",
      "Epoch 73/150 - loss: 0.1559 - f1-score: 94.43% - val_loss: 0.9272 - val_f1-score: 72.94% - time: 103.29s\n",
      "Epoch 74/150 - loss: 0.1565 - f1-score: 94.55% - val_loss: 0.9227 - val_f1-score: 72.85% - time: 105.51s\n",
      "Epoch 75/150 - loss: 0.1563 - f1-score: 94.65% - val_loss: 0.9296 - val_f1-score: 72.61% - time: 104.65s\n",
      "Epoch 76/150 - loss: 0.1543 - f1-score: 94.67% - val_loss: 0.9223 - val_f1-score: 72.96% - time: 104.07s\n",
      "Epoch 77/150 - loss: 0.1546 - f1-score: 94.59% - val_loss: 0.9187 - val_f1-score: 72.80% - time: 105.33s\n",
      "Epoch 78/150 - loss: 0.1557 - f1-score: 94.53% - val_loss: 0.9305 - val_f1-score: 73.11% - time: 105.60s\n",
      "Epoch 79/150 - loss: 0.1567 - f1-score: 94.64% - val_loss: 0.9210 - val_f1-score: 72.91% - time: 103.93s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150 - loss: 0.1553 - f1-score: 94.53% - val_loss: 0.9278 - val_f1-score: 72.97% - time: 103.59s\n",
      "Epoch 81/150 - loss: 0.1539 - f1-score: 94.69% - val_loss: 0.9168 - val_f1-score: 72.85% - time: 103.71s\n",
      "Epoch 82/150 - loss: 0.1553 - f1-score: 94.61% - val_loss: 0.9187 - val_f1-score: 72.83% - time: 103.17s\n",
      "Epoch 83/150 - loss: 0.1556 - f1-score: 94.45% - val_loss: 0.9222 - val_f1-score: 73.03% - time: 103.84s\n",
      "Epoch 84/150 - loss: 0.1553 - f1-score: 94.67% - val_loss: 0.9218 - val_f1-score: 72.93% - time: 102.88s\n",
      "Epoch 85/150 - loss: 0.1536 - f1-score: 94.56% - val_loss: 0.9243 - val_f1-score: 72.75% - time: 104.82s\n",
      "Epoch 86/150 - loss: 0.1516 - f1-score: 94.64% - val_loss: 0.9145 - val_f1-score: 72.97% - time: 104.90s\n",
      "Epoch 87/150 - loss: 0.1544 - f1-score: 94.59% - val_loss: 0.9193 - val_f1-score: 73.12% - time: 102.37s\n",
      "Epoch 88/150 - loss: 0.1574 - f1-score: 94.44% - val_loss: 0.9202 - val_f1-score: 72.84% - time: 103.24s\n",
      "Epoch 89/150 - loss: 0.1542 - f1-score: 94.48% - val_loss: 0.9247 - val_f1-score: 72.99% - time: 103.95s\n",
      "Epoch 90/150 - loss: 0.1544 - f1-score: 94.56% - val_loss: 0.9275 - val_f1-score: 72.98% - time: 103.63s\n",
      "Epoch 91/150 - loss: 0.1551 - f1-score: 94.59% - val_loss: 0.9211 - val_f1-score: 72.71% - time: 103.16s\n",
      "Epoch 92/150 - loss: 0.1547 - f1-score: 94.52% - val_loss: 0.9232 - val_f1-score: 73.06% - time: 104.18s\n",
      "Epoch 93/150 - loss: 0.1526 - f1-score: 94.71% - val_loss: 0.9205 - val_f1-score: 72.96% - time: 103.58s\n",
      "Epoch 94/150 - loss: 0.1541 - f1-score: 94.65% - val_loss: 0.9216 - val_f1-score: 73.19% - time: 104.77s\n",
      "Epoch 95/150 - loss: 0.1527 - f1-score: 94.63% - val_loss: 0.9234 - val_f1-score: 72.94% - time: 112.53s\n",
      "Epoch 96/150 - loss: 0.1525 - f1-score: 94.61% - val_loss: 0.9155 - val_f1-score: 73.07% - time: 138.10s\n",
      "Epoch 97/150 - loss: 0.1576 - f1-score: 94.43% - val_loss: 0.9281 - val_f1-score: 72.92% - time: 113.71s\n",
      "Epoch 98/150 - loss: 0.1552 - f1-score: 94.57% - val_loss: 0.9223 - val_f1-score: 72.83% - time: 106.10s\n",
      "Epoch 99/150 - loss: 0.1557 - f1-score: 94.66% - val_loss: 0.9183 - val_f1-score: 72.99% - time: 103.95s\n",
      "Epoch 100/150 - loss: 0.1556 - f1-score: 94.45% - val_loss: 0.9178 - val_f1-score: 72.73% - time: 103.52s\n",
      "Epoch 101/150 - loss: 0.1570 - f1-score: 94.54% - val_loss: 0.9216 - val_f1-score: 73.15% - time: 103.85s\n",
      "Epoch 102/150 - loss: 0.1533 - f1-score: 94.53% - val_loss: 0.9182 - val_f1-score: 73.26% - time: 103.11s\n",
      "Epoch 103/150 - loss: 0.1546 - f1-score: 94.57% - val_loss: 0.9179 - val_f1-score: 72.83% - time: 104.04s\n",
      "Epoch 104/150 - loss: 0.1547 - f1-score: 94.57% - val_loss: 0.9281 - val_f1-score: 72.83% - time: 103.66s\n",
      "Epoch 105/150 - loss: 0.1531 - f1-score: 94.67% - val_loss: 0.9211 - val_f1-score: 73.00% - time: 115.36s\n",
      "Epoch 106/150 - loss: 0.1525 - f1-score: 94.65% - val_loss: 0.9162 - val_f1-score: 73.20% - time: 125.76s\n",
      "Epoch 107/150 - loss: 0.1591 - f1-score: 94.46% - val_loss: 0.9278 - val_f1-score: 73.03% - time: 113.35s\n",
      "Epoch 108/150 - loss: 0.1555 - f1-score: 94.54% - val_loss: 0.9265 - val_f1-score: 72.95% - time: 133.66s\n",
      "Epoch 109/150 - loss: 0.1574 - f1-score: 94.39% - val_loss: 0.9275 - val_f1-score: 72.97% - time: 107.07s\n",
      "Epoch 110/150 - loss: 0.1566 - f1-score: 94.51% - val_loss: 0.9189 - val_f1-score: 73.02% - time: 104.37s\n",
      "Epoch 111/150 - loss: 0.1542 - f1-score: 94.64% - val_loss: 0.9217 - val_f1-score: 72.95% - time: 104.53s\n",
      "Epoch 112/150 - loss: 0.1539 - f1-score: 94.62% - val_loss: 0.9206 - val_f1-score: 73.14% - time: 103.75s\n",
      "Epoch 113/150 - loss: 0.1579 - f1-score: 94.46% - val_loss: 0.9278 - val_f1-score: 72.68% - time: 105.33s\n",
      "Epoch 114/150 - loss: 0.1533 - f1-score: 94.65% - val_loss: 0.9117 - val_f1-score: 72.92% - time: 102.18s\n",
      "Epoch 115/150 - loss: 0.1535 - f1-score: 94.66% - val_loss: 0.9228 - val_f1-score: 72.99% - time: 103.97s\n",
      "Epoch 116/150 - loss: 0.1527 - f1-score: 94.64% - val_loss: 0.9237 - val_f1-score: 73.22% - time: 103.11s\n",
      "Epoch 117/150 - loss: 0.1546 - f1-score: 94.52% - val_loss: 0.9267 - val_f1-score: 73.15% - time: 104.76s\n",
      "Epoch 118/150 - loss: 0.1535 - f1-score: 94.57% - val_loss: 0.9224 - val_f1-score: 72.82% - time: 181.62s\n",
      "Epoch 119/150 - loss: 0.1538 - f1-score: 94.63% - val_loss: 0.9183 - val_f1-score: 73.00% - time: 175.08s\n",
      "Epoch 120/150 - loss: 0.1561 - f1-score: 94.43% - val_loss: 0.9244 - val_f1-score: 73.19% - time: 116.66s\n",
      "Epoch 121/150 - loss: 0.1557 - f1-score: 94.61% - val_loss: 0.9296 - val_f1-score: 72.84% - time: 117.17s\n",
      "Epoch 122/150 - loss: 0.1563 - f1-score: 94.70% - val_loss: 0.9296 - val_f1-score: 72.95% - time: 116.45s\n",
      "Epoch 123/150 - loss: 0.1540 - f1-score: 94.70% - val_loss: 0.9287 - val_f1-score: 72.93% - time: 106.59s\n",
      "Epoch 124/150 - loss: 0.1540 - f1-score: 94.63% - val_loss: 0.9169 - val_f1-score: 72.98% - time: 104.60s\n",
      "Epoch 125/150 - loss: 0.1542 - f1-score: 94.56% - val_loss: 0.9295 - val_f1-score: 72.63% - time: 128.94s\n",
      "Epoch 126/150 - loss: 0.1542 - f1-score: 94.67% - val_loss: 0.9300 - val_f1-score: 73.11% - time: 106.66s\n",
      "Epoch 127/150 - loss: 0.1511 - f1-score: 94.78% - val_loss: 0.9222 - val_f1-score: 72.87% - time: 107.70s\n",
      "Epoch 128/150 - loss: 0.1532 - f1-score: 94.62% - val_loss: 0.9350 - val_f1-score: 73.09% - time: 147.83s\n",
      "Epoch 129/150 - loss: 0.1566 - f1-score: 94.60% - val_loss: 0.9221 - val_f1-score: 72.94% - time: 115.11s\n",
      "Epoch 130/150 - loss: 0.1550 - f1-score: 94.48% - val_loss: 0.9178 - val_f1-score: 72.80% - time: 105.16s\n",
      "Epoch 131/150 - loss: 0.1556 - f1-score: 94.40% - val_loss: 0.9192 - val_f1-score: 72.92% - time: 103.77s\n",
      "Epoch 132/150 - loss: 0.1554 - f1-score: 94.55% - val_loss: 0.9381 - val_f1-score: 72.57% - time: 103.60s\n",
      "Epoch 133/150 - loss: 0.1531 - f1-score: 94.72% - val_loss: 0.9229 - val_f1-score: 72.85% - time: 103.75s\n",
      "Epoch 134/150 - loss: 0.1568 - f1-score: 94.33% - val_loss: 0.9414 - val_f1-score: 72.55% - time: 104.27s\n",
      "Epoch 135/150 - loss: 0.1544 - f1-score: 94.58% - val_loss: 0.9232 - val_f1-score: 72.68% - time: 114.98s\n",
      "Epoch 136/150 - loss: 0.1565 - f1-score: 94.67% - val_loss: 0.9336 - val_f1-score: 72.56% - time: 141.99s\n",
      "Epoch 137/150 - loss: 0.1553 - f1-score: 94.57% - val_loss: 0.9236 - val_f1-score: 73.04% - time: 115.04s\n",
      "Epoch 138/150 - loss: 0.1531 - f1-score: 94.74% - val_loss: 0.9256 - val_f1-score: 72.79% - time: 107.60s\n",
      "Epoch 139/150 - loss: 0.1549 - f1-score: 94.60% - val_loss: 0.9248 - val_f1-score: 73.03% - time: 111.08s\n",
      "Epoch 140/150 - loss: 0.1557 - f1-score: 94.64% - val_loss: 0.9251 - val_f1-score: 73.07% - time: 113.25s\n",
      "Epoch 141/150 - loss: 0.1549 - f1-score: 94.50% - val_loss: 0.9290 - val_f1-score: 73.00% - time: 105.34s\n",
      "Epoch 142/150 - loss: 0.1533 - f1-score: 94.70% - val_loss: 0.9249 - val_f1-score: 73.11% - time: 105.54s\n",
      "Epoch 143/150 - loss: 0.1542 - f1-score: 94.46% - val_loss: 0.9163 - val_f1-score: 73.16% - time: 148.79s\n",
      "Epoch 144/150 - loss: 0.1549 - f1-score: 94.54% - val_loss: 0.9225 - val_f1-score: 72.78% - time: 105.37s\n",
      "Epoch 145/150 - loss: 0.1551 - f1-score: 94.53% - val_loss: 0.9271 - val_f1-score: 73.26% - time: 105.41s\n",
      "Epoch 146/150 - loss: 0.1560 - f1-score: 94.55% - val_loss: 0.9261 - val_f1-score: 72.58% - time: 104.17s\n",
      "Epoch 147/150 - loss: 0.1552 - f1-score: 94.50% - val_loss: 0.9156 - val_f1-score: 72.94% - time: 103.60s\n",
      "Epoch 148/150 - loss: 0.1541 - f1-score: 94.48% - val_loss: 0.9273 - val_f1-score: 72.78% - time: 103.86s\n",
      "Epoch 149/150 - loss: 0.1540 - f1-score: 94.61% - val_loss: 0.9240 - val_f1-score: 72.90% - time: 103.50s\n",
      "Epoch 150/150 - loss: 0.1557 - f1-score: 94.53% - val_loss: 0.9209 - val_f1-score: 72.96% - time: 103.45s\n",
      "\n",
      "Damage_Severity_Classification Performance:\n",
      "Accuracy : 75.21%\n",
      "Precision : 73.73%\n",
      "Recall : 75.21%\n",
      "F1-Score : 74.21%\n",
      "\n",
      "Damage_Severity_Classification Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      2135\n",
      "           1       0.42      0.36      0.39       629\n",
      "           2       0.70      0.72      0.71      1101\n",
      "\n",
      "    accuracy                           0.73      3865\n",
      "   macro avg       0.65      0.64      0.65      3865\n",
      "weighted avg       0.73      0.73      0.73      3865\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_f1</td><td>▁▂▄▅▆▇██████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅███▇▅▆▇▇▆▆▆▇▆▅▆▆▆▆▆▆▆▆▇▇▆▆▆▇▆▆▆▇▆▆▆▇▆▆</td></tr><tr><td>val_loss</td><td>▂▁▁▂▄▇▇▇▇▇▇███▇█▇█▇██████▇▇███▇███▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_f1</td><td>94.53452</td></tr><tr><td>train_loss</td><td>0.15566</td></tr><tr><td>val_f1</td><td>72.9616</td></tr><tr><td>val_loss</td><td>0.92091</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mobilenet-v2-dsreproduce</strong> at: <a href='https://wandb.ai/lmc6130/Reproduce_exp/runs/poskci71' target=\"_blank\">https://wandb.ai/lmc6130/Reproduce_exp/runs/poskci71</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231025_171729-poskci71/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_ps = 0\n",
    "best_val_rs = 0\n",
    "best_val_f1 = 0\n",
    "\n",
    "output_list = [] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, train_ps, train_rs, train_f1, train_time = train_one_epoch(trainloader) \n",
    "    dev_loss, dev_acc = dev_one_epoch(devloader) \n",
    "    val_loss, val_acc, val_ps, val_rs, val_f1, val_time, best_val_f1, cm, cr = val_one_epoch(testloader, best_val_f1) \n",
    "    \n",
    "    lr_scheduler.step(dev_acc)\n",
    "\n",
    "    if val_ps > best_val_ps:\n",
    "        best_val_ps = val_ps\n",
    "\n",
    "    if val_rs > best_val_rs:\n",
    "        best_val_rs = val_rs\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "    total_time = train_time + val_time\n",
    "    output_str = f\"Epoch {epoch+1}/{epochs} - loss: {train_loss:.4f} - f1-score: {train_f1:.2f}% - val_loss: {val_loss:.4f} - val_f1-score: {val_f1:.2f}% - time: {total_time:.2f}s\"\n",
    "    output_list.append(output_str)\n",
    "    print(output_str)\n",
    "\n",
    "print()\n",
    "print(task + ' Performance:')\n",
    "print(f'Accuracy : {best_val_acc:.2f}%')\n",
    "print(f'Precision : {best_val_ps:.2f}%')\n",
    "print(f'Recall : {best_val_rs:.2f}%')\n",
    "print(f'F1-Score : {best_val_f1:.2f}%')\n",
    "print()\n",
    "print(task + ' Classification Report:')\n",
    "print(cr)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=None)\n",
    "disp.plot()\n",
    "plt.savefig(\"reproduce-CM.png\") \n",
    "plt.close() \n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
