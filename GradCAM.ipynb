{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.nn.parallel import DataParallel\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    parallel = True\n",
    "else:\n",
    "    parallel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28319, 4)\n",
      "(3865, 4)\n",
      "(2712, 4)\n"
     ]
    }
   ],
   "source": [
    "train_path = 'C:/crisis_vision_benchmarks/tasks/damage_severity/consolidated/consolidated_damage_train_final.tsv'\n",
    "test_path = 'C:/crisis_vision_benchmarks/tasks/damage_severity/consolidated/consolidated_damage_test_final.tsv'\n",
    "dev_path = 'C:/crisis_vision_benchmarks/tasks/damage_severity/consolidated/consolidated_damage_dev_final.tsv'\n",
    "\n",
    "train_label = pd.read_table(train_path)\n",
    "test_label = pd.read_table(test_path)\n",
    "dev_label = pd.read_table(dev_path)\n",
    "\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)\n",
    "print(dev_label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3865/3865 [00:41<00:00, 92.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "(3865, 3, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3865/3865 [00:00<00:00, 773166.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3865,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_label_map = {\"severe\": 2, \"mild\": 1, \"little_or_none\": 0}\n",
    "\n",
    "def get_data(pd_label, class_label_map):\n",
    "    image_path = pd_label['image_path']\n",
    "    class_label = pd_label['class_label']\n",
    "\n",
    "    X, y, valid_indices = [], [], []\n",
    "\n",
    "    tfms = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    for i in tqdm(range(len(image_path))):\n",
    "        path = os.path.join('C:/crisis_vision_benchmarks/', image_path[i])\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            if img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = tfms(img) \n",
    "            X.append(img)\n",
    "            valid_indices.append(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image: {path} - {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(X[0].shape)\n",
    "    X = np.stack(X)\n",
    "    print(X.shape)\n",
    "\n",
    "    for idx in tqdm(valid_indices):\n",
    "        label = class_label[idx]\n",
    "        if label not in class_label_map:\n",
    "            print(f\"Error: Unknown class label: {label}\")\n",
    "            continue\n",
    "        y.append(class_label_map[label])\n",
    "\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    print(y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_test, y_test = get_data(test_label, class_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "testset = TensorDataset(X_test, y_test)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(num_ftrs, len(class_label_map))\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCamModel(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        self.gradients = None \n",
    "        self.tensorhook = [] \n",
    "        self.layerhook = [] \n",
    "        self.selected_out = None \n",
    " \n",
    "        #PRETRAINED MODEL \n",
    "        self.pretrained = model\n",
    "        self.layerhook.append(self.pretrained.features[-1].register_forward_hook(self.forward_hook())) \n",
    " \n",
    "        for p in self.pretrained.parameters(): \n",
    "            p.requires_grad = True \n",
    " \n",
    "    def activations_hook(self,grad): \n",
    "        self.gradients = grad \n",
    " \n",
    "    def get_act_grads(self): \n",
    "        return self.gradients \n",
    " \n",
    "    def forward_hook(self): \n",
    "        def hook(module, inp, out): \n",
    "            self.selected_out = out \n",
    "            self.tensorhook.append(out.register_hook(self.activations_hook)) \n",
    "        return hook \n",
    " \n",
    "    def forward(self,x): \n",
    "        out = self.pretrained(x) \n",
    "        return out, self.selected_out\n",
    "    \n",
    "gcmodel = GradCamModel().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'D:/Research/Exploration/crisis_vision_benchmarks/gradcam'\n",
    "image_index = 1\n",
    "\n",
    "for images, labels in testloader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    preds = model(images)\n",
    "\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    out, acts = gcmodel(images) \n",
    "    acts = acts.detach().cpu() \n",
    "    loss = nn.CrossEntropyLoss()(out, labels).to(device)\n",
    "    loss.backward() \n",
    "    grads = gcmodel.get_act_grads().detach().cpu() \n",
    "    pooled_grads = torch.mean(grads, dim=[0,2,3]).detach().cpu()\n",
    "\n",
    "    predicted = predicted.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    predicted_class = [k for k, v in class_label_map.items() if v == int(predicted[0])][0]\n",
    "    true_class = [k for k, v in class_label_map.items() if v == int(labels[0])][0]\n",
    "    #print(f\"Predicted: {predicted_class}, Ground Truth: {true_class}\")\n",
    "\n",
    "    for i in range(acts.shape[1]):\n",
    "        acts[:,i,:,:] += pooled_grads[i]\n",
    "        \n",
    "    heatmap_j = torch.mean(acts, dim = 1).squeeze()\n",
    "    heatmap_j_max = heatmap_j.max(axis = 0)[0]\n",
    "    heatmap_j /= heatmap_j_max\n",
    "    heatmap_j = heatmap_j.detach().cpu().numpy()\n",
    "\n",
    "    for i in range(len(heatmap_j)):\n",
    "        heatmap_j_i = cv2.resize(heatmap_j[i],(224, 224))\n",
    "        cmap = mpl.colormaps['inferno']\n",
    "        heatmap_j2 = cmap(heatmap_j_i, alpha = 0.5)\n",
    "        print(f\"Predicted: {predicted_class}, Ground Truth: {true_class}\")\n",
    "        fig, axs = plt.subplots(1,1,figsize = (5,5))\n",
    "        axs.axis('off')\n",
    "        image_np = np.transpose(images.detach().cpu().numpy()[i], [1, 2, 0])\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "        image_np = (image_np - np.min(image_np)) / (np.max(image_np) - np.min(image_np))\n",
    "        axs.imshow(image_np)\n",
    "        axs.imshow(heatmap_j2)\n",
    "        plt.savefig(os.path.join(output_folder, f'GradCAM_{image_index}.jpg'))\n",
    "        image_index += 1\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
